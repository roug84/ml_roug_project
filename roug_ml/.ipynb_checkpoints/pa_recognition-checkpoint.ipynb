{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'roug_ml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbeartype\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroug_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_utl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calc_loss_acc_val\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroug_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_multiclass_confusion_matrix\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroug_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlflow_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_best_run\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'roug_ml'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from beartype.typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roug_ml.utl.evaluation.eval_utl import calc_loss_acc_val\n",
    "from roug_ml.utl.evaluation.multiclass import compute_multiclass_confusion_matrix\n",
    "from roug_ml.utl.mlflow_utils import get_best_run\n",
    "from roug_ml.utl.paths_utl import create_dir\n",
    "from roug_ml.models.hyperoptimization import generate_param_grid_with_different_size_layers, \\\n",
    "    get_or_create_experiment, get_best_run_from_hyperoptim\n",
    "from roug_ml.utl.processing.filters import LowPassFilter\n",
    "from roug_ml.utl.processing.signal_processing import SignalProcessor\n",
    "from roug_ml.utl.transforms.features_management import FeatureFlattener\n",
    "from set_seed import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from roug_ml.models.hyperoptimization import parallele_hyper_optim\n",
    "from parameter_utils import restructure_dict\n",
    "from processing.dataset_processing import covert_3a_from_pandas_to_dict\n",
    "from etl.data_extraction import extract_activity_data_from_users\n",
    "from roug_ml.models.pipelines.pipelines import NNTorch\n",
    "from roug_ml.configs.data_paths import M_HEALTH_PATH\n",
    "from roug_ml.configs.data_labels import M_HEALTH_ACTIVITIES_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_functions = {\n",
    "    \"categorical_crossentropy\": \"categorical_crossentropy\"\n",
    "}\n",
    "\n",
    "\n",
    "def filter_data_dict(data_dict: dict, subjects: list) -> dict:\n",
    "    \"\"\"\n",
    "    Filter the data_dict dictionary based on a specific set of subjects.\n",
    "    :param data_dict: The input dictionary.\n",
    "    :param subjects: The list of subjects to keep.\n",
    "    returns: The filtered dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dict = {\n",
    "        key: [value[i] for i in range(len(data_dict['user'])) if data_dict['user'][i] in subjects]\n",
    "        for key, value in data_dict.items()}\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "class MHealtAnalysis:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Performs following analysis:\n",
    "            - Extract data\n",
    "            - Make plots for analysis\n",
    "        \"\"\"\n",
    "        self.list_of_positions = ['chest', 'ankle', 'right_arm']\n",
    "        # self.list_of_positions = ['right_arm']\n",
    "        self.number_of_point_per_activity = 511  # TODO: automatic\n",
    "\n",
    "        self.features_to_extract = ['mean', 'std', 'rms', 'max', 'min', 'var']\n",
    "\n",
    "        # Define the number of parallel workers\n",
    "        # Use all available CPU cores except one\n",
    "        self.num_workers = 1  # multiprocessing.cpu_count() - 1\n",
    "        self.framework = 'torch'\n",
    "        # self.framework = 'tf'\n",
    "\n",
    "        # model params:\n",
    "        self.nn_params_keys = [\"activations\", \"in_nn\", \"input_shape\", \"output_shape\"]\n",
    "        self.other_keys = [\"batch_size\", \"cost_function\", \"learning_rate\", \"metrics\", \"n_epochs\",\n",
    "                           \"nn_key\"]\n",
    "\n",
    "        self.results_path = os.path.join(M_HEALTH_PATH, '../models')\n",
    "        create_dir(self.results_path)\n",
    "\n",
    "        mlflow.set_tracking_uri(\"http://localhost:8000\")\n",
    "        self.mlflow_experiment_name = 'run_non_parallel_shu_5'\n",
    "        self.mlflow_experiment_id = get_or_create_experiment(self.mlflow_experiment_name)\n",
    "\n",
    "        self.re_optimize = True\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        dataset = self.read_data(M_HEALTH_PATH)\n",
    "\n",
    "        # Process data to right format\n",
    "        dataset_dict_chest = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                           in_label_col=23,\n",
    "                                                           in_user_col=24,\n",
    "                                                           in_3a_col=[0, 1, 2])\n",
    "\n",
    "        dataset_dict_ankle = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                           in_label_col=23,\n",
    "                                                           in_user_col=24,\n",
    "                                                           in_3a_col=[5, 6, 7])\n",
    "\n",
    "        dataset_dict_right_arm = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                               in_label_col=23,\n",
    "                                                               in_user_col=24,\n",
    "                                                               in_3a_col=[14, 15, 16])\n",
    "\n",
    "        dataset_dict = {\n",
    "            'y_num': dataset_dict_chest['y_num'],\n",
    "            'y_onehot': dataset_dict_chest['y_onehot'],\n",
    "            'x_chest': dataset_dict_chest['x'],\n",
    "            'x_ankle': dataset_dict_ankle['x'],\n",
    "            'x_right_arm': dataset_dict_right_arm['x'],\n",
    "            'User': dataset_dict_chest['User']\n",
    "        }\n",
    "\n",
    "        # Create dataset\n",
    "        final_data_set = self.extract_data_during_activity(\n",
    "            in_num_of_users=10,\n",
    "            in_dataset_dict=dataset_dict,\n",
    "            in_accelerometer_position=self.list_of_positions)\n",
    "\n",
    "        data_set_train, data_set_test = self.split_data_for_train_test_portion_of_pa(\n",
    "            in_data=final_data_set, in_points_in_each_set=self.number_of_point_per_activity,\n",
    "            in_accelerometer_position=self.list_of_positions)\n",
    "\n",
    "        # Split by patients: in the original dataset we use\n",
    "        subjects_to_keep = ['subject' + str(i) for i in range(7)]\n",
    "        data_set_train = filter_data_dict(data_set_train, subjects_to_keep)\n",
    "\n",
    "        subjects_to_keep = ['subject' + str(i + 7) for i in range(3)]\n",
    "        data_set_test = filter_data_dict(data_set_test, subjects_to_keep)\n",
    "\n",
    "        key_position = 'x_' + 'right_arm'\n",
    "        # key_position = 'x_' + 'chest'\n",
    "        # key_position = 'x_' + 'ankle'\n",
    "\n",
    "        x_train, y_train_oh = self.create_x_y(data_set_train, in_key=key_position + '_train',\n",
    "                                              in_features_to_extract=self.features_to_extract,\n",
    "                                              sample_freq=50,\n",
    "                                              window_size=1,\n",
    "                                              cutoff_frequency=5\n",
    "                                              )\n",
    "\n",
    "        x_val, y_val_oh = self.create_x_y(data_set_test, in_key=key_position + '_test',\n",
    "                                          in_features_to_extract=self.features_to_extract,\n",
    "                                          sample_freq=50,\n",
    "                                          window_size=1,\n",
    "                                          cutoff_frequency=5)\n",
    "\n",
    "        # Reshape the input data to match the expected shape of the model\n",
    "        x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "        x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "        y = y_train_oh\n",
    "        y_val = y_val_oh\n",
    "\n",
    "        if self.re_optimize:\n",
    "            # Optimize and get best run\n",
    "            params = self.generate_params(x_train=x_train)\n",
    "            results = self.hyperoptimize(x_train, y, x_val, y_val, params)\n",
    "            best_params, best_val_accuracy, best_run_id = get_best_run_from_hyperoptim(results)\n",
    "        else:\n",
    "            # Get the best run from mlopt\n",
    "            mlflow.set_tracking_uri(\"http://localhost:8000\")\n",
    "            best_run_id, best_params = get_best_run(self.mlflow_experiment_name, \"val_accuracy\")\n",
    "\n",
    "            best_params = restructure_dict(best_params, self.nn_params_keys, self.other_keys,\n",
    "                                           in_from_mlflow=True)\n",
    "\n",
    "        # Load the best model\n",
    "        best_model = mlflow.pytorch.load_model(\"runs:/{}/models\".format(best_run_id))\n",
    "\n",
    "        pipeline_torch = Pipeline(steps=[('NN', NNTorch(**best_params))])\n",
    "        pipeline_torch.named_steps['NN'].nn_model = best_model\n",
    "        #\n",
    "        predictions = pipeline_torch.predict(x_val)\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        y_val_binary = np.argmax(y_val, axis=1)\n",
    "        val_acc = calc_loss_acc_val(predictions, y_val_binary)\n",
    "        print(val_acc)\n",
    "        compute_multiclass_confusion_matrix(targets=y_val_binary,\n",
    "                                            outputs=predictions,\n",
    "                                            class_labels=M_HEALTH_ACTIVITIES_LABELS\n",
    "                                            )\n",
    "\n",
    "    def generate_params(self, x_train):\n",
    "        \"\"\"\n",
    "        This function generates the list of parameters for the hyperparameter optimization.\n",
    "\n",
    "        :return: list_params: list of dictionaries, where each dictionary contains a unique\n",
    "         combination of hyperparameters\n",
    "        \"\"\"\n",
    "        # Define hyperparameters\n",
    "        nn_key = ['CNN']\n",
    "        input_shape = [np.asarray(x_train).shape[1]]\n",
    "        output_shape = [13]\n",
    "        batch_size = [  # 32, 64,\n",
    "            128]\n",
    "        cost_function = [loss_functions['categorical_crossentropy']]\n",
    "        learning_rate = [  # 0.01,\n",
    "            0.001]\n",
    "        n_epochs = [  # 300, 150, 140, 130, 120,\n",
    "            100]\n",
    "        metrics = ['accuracy']\n",
    "        layer_sizes = [  # [200, 300], [200, 300, 100], [200, 300, 400, 100], [50, 300, 100, 50],\n",
    "            # [100, 200, 200, 100], [50, 100, 100, 100, 50],\n",
    "            [100, 200, 100, 200, 100]\n",
    "        ]\n",
    "        activations = [\n",
    "            # ['relu', 'relu'], ['relu', 'tanh'], ['relu', 'relu', 'relu'],\n",
    "            # ['tanh', 'tanh', 'tanh', 'tanh'], ['relu', 'relu', 'relu', 'relu'],\n",
    "            # ['relu', 'relu', 'relu', 'relu', 'tanh'],\n",
    "            ['relu', 'relu', 'relu', 'relu', 'relu']\n",
    "        ]\n",
    "        cnn_filters = [1, 3, 5, 10]\n",
    "\n",
    "        list_params = generate_param_grid_with_different_size_layers(nn_key, input_shape,\n",
    "                                                                     output_shape, batch_size,\n",
    "                                                                     cost_function, learning_rate,\n",
    "                                                                     n_epochs, metrics,\n",
    "                                                                     layer_sizes, activations,\n",
    "                                                                     cnn_filters)\n",
    "\n",
    "        list_params = [\n",
    "            restructure_dict(params, self.nn_params_keys, self.other_keys, in_from_mlflow=False) for\n",
    "            params in list_params]\n",
    "\n",
    "        return list_params\n",
    "\n",
    "    def hyperoptimize(self, x_train, y, x_val, y_val, list_params):\n",
    "        \"\"\"\n",
    "        This function performs hyperparameter optimization using parallel computing.\n",
    "\n",
    "        :param x_train: array-like, shape (n_samples, n_features), input training data\n",
    "        :param y: array-like, shape (n_samples, ), target training values\n",
    "        :param x_val: array-like, shape (n_samples, n_features), input validation data\n",
    "        :param y_val: array-like, shape (n_samples, ), target validation values\n",
    "        :param list_params: list of dictionaries, where each dictionary contains a unique\n",
    "        combination of hyperparameters\n",
    "\n",
    "        :return: results: list of tuples, each containing the parameters, validation accuracy, and\n",
    "         run ID for one run of the model\n",
    "        \"\"\"\n",
    "        # Perform hyperparameter optimization\n",
    "        results = parallele_hyper_optim(in_num_workers=self.num_workers,\n",
    "                                        x_train=x_train,\n",
    "                                        y=y,\n",
    "                                        x_val=x_val,\n",
    "                                        y_val=y_val,\n",
    "                                        param_grid_outer=list_params,\n",
    "                                        in_framework=self.framework,\n",
    "                                        model_save_path=self.results_path,\n",
    "                                        in_mlflow_experiment_name=self.mlflow_experiment_id\n",
    "                                        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def create_x_y(final_data_set: dict,\n",
    "                   in_key: str,\n",
    "                   in_features_to_extract: list,\n",
    "                   sample_freq: int,\n",
    "                   window_size: float,\n",
    "                   cutoff_frequency: int):\n",
    "        \"\"\"\n",
    "        Prepare the data by generating the x_data and y_hot_data arrays.\n",
    "        :param final_data_set: The final data set containing the input data.\n",
    "        :param in_key: The key to access the input data in the final_data_set.\n",
    "        :param in_features_to_extract: A list of features to extract from the data. These features\n",
    "                are extracted from final_data_set[in_key] and used as input (x_train).\n",
    "        :param sample_freq: The sample frequency in Hz.\n",
    "        :param window_size: The size of the window in seconds.\n",
    "        :param cutoff_frequency: The cutoff frequency for low-pass filtering.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the x_train and y_train_oh arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        sampling_rate = 1 / sample_freq\n",
    "        points_for_mean = int(window_size / sampling_rate)\n",
    "\n",
    "        dataset_features = []\n",
    "        for x in final_data_set[in_key]:\n",
    "            sp = SignalProcessor(input_signal=x,\n",
    "                                 in_filter=(LowPassFilter, {'cutoff_frequency': cutoff_frequency}),\n",
    "                                 in_window_size=points_for_mean,\n",
    "                                 in_stride=points_for_mean\n",
    "                                 )\n",
    "            sp.apply_filter(in_signal=sp.input_signal)\n",
    "            # sp.calibrate_data(in_signal=sp.output_signal)\n",
    "            features = sp.extract_windowed_features(in_signal=sp.output_signal)\n",
    "            dataset_features.append(features)\n",
    "\n",
    "        flattener = FeatureFlattener(final_data_set, in_features_to_extract)\n",
    "        new_feat = flattener.run(dataset_features)\n",
    "\n",
    "        final_data_set['new_feat'] = np.asarray(new_feat)\n",
    "\n",
    "        x_data = final_data_set['new_feat']\n",
    "        y_hot_data = np.asarray(final_data_set['y'])\n",
    "\n",
    "        return x_data, y_hot_data\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(in_path: str):\n",
    "        \"\"\"\n",
    "        read data\n",
    "        :param in_path: the path\n",
    "        \"\"\"\n",
    "        _, _, filenames = next(os.walk(in_path))\n",
    "        total_df = []\n",
    "        for file_x in filenames:\n",
    "            if file_x.endswith(\".log\"):\n",
    "                print(file_x[8:-4])\n",
    "                # Jutar preprocessed_path \\ file_x\n",
    "                path_to_read = os.path.join(in_path, file_x)\n",
    "                #  leer archivo path_to_read\n",
    "                input_df = pd.read_csv(path_to_read, delimiter=\"\\t\", header=None)\n",
    "                input_df[24] = file_x[8:-4]\n",
    "                total_df.append(input_df[[0, 1, 2, 5, 6, 7, 14, 15, 16, 23, 24]])\n",
    "        return pd.concat(total_df, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_train_test(data: np.array, in_points_in_each_set: int):\n",
    "        \"\"\"\n",
    "        Splits the data into a training set and a test set.\n",
    "\n",
    "        Parameters:\n",
    "        data (np.array): The data to be split.\n",
    "        in_points_in_each_set (int): Number of points in each dataset.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Contains arrays for training data and test data.\n",
    "        \"\"\"\n",
    "        # Convert 3a to image for each position\n",
    "        train_data = data[0:in_points_in_each_set]\n",
    "        test_data = data[in_points_in_each_set + 1: 2 * in_points_in_each_set + 1]\n",
    "        return train_data, test_data\n",
    "\n",
    "    def extract_data_during_activity(self,\n",
    "                                     in_num_of_users: int = 10,\n",
    "                                     in_dataset_dict: Dict = None,\n",
    "                                     in_accelerometer_position: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract data from physical activities periods (Label > 0) from dataset.\n",
    "        :param in_num_of_users: number of patients\n",
    "        :param in_dataset_dict: dictionary with data\n",
    "        :param in_accelerometer_position: position of accelerometer in the body\n",
    "        :return: dictionary final_data_set with train test data.\n",
    "        \"\"\"\n",
    "\n",
    "        data_keys = [f\"x_{key}\" for key in in_accelerometer_position]\n",
    "        final_data_set = {key: [] for key in ['y_label', 'y_onehot', 'user', *data_keys]}\n",
    "\n",
    "        list_of_classes = in_dataset_dict['y_num']\n",
    "        for label in np.unique(list_of_classes):\n",
    "            # Number of users\n",
    "            list_user = ['subject' + str(i + 1) for i in range(in_num_of_users)]\n",
    "            for user_i in list_user:\n",
    "                user_data_dict = extract_activity_data_from_users(\n",
    "                    user_i, label, in_dataset_dict, self.list_of_positions)\n",
    "\n",
    "                for key in in_accelerometer_position:\n",
    "                    final_data_set[f\"x_{key}\"].append(user_data_dict[f\"{key}_label\"])\n",
    "\n",
    "                final_data_set['y_label'].append(label)\n",
    "                final_data_set['y_onehot'].append(user_data_dict['y_onehot'][0])\n",
    "                final_data_set['user'].append(user_i)\n",
    "        return final_data_set\n",
    "\n",
    "    def split_data_for_train_test_portion_of_pa(self,\n",
    "                                                in_data: Dict,\n",
    "                                                in_points_in_each_set: int = int(1022 / 2),\n",
    "                                                in_accelerometer_position=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Split data for training and test sets. in_points_in_each_set for training and\n",
    "        in_points_in_each_set for testing. Data is already a dict of lists where x[0] is a set of 3a\n",
    "        date that correspond to label y_label[0] and its one_hot_version (y_onehot[0]). Therefore,\n",
    "        the only variable that is processed (split) here is x\n",
    "        :param in_data: dictionary with data\n",
    "        :param in_points_in_each_set: int, number of points in the dataset\n",
    "        :param in_accelerometer_position: position of accelerometer in the body\n",
    "        :return: dictionary with train test data.\n",
    "        \"\"\"\n",
    "        # TODO: split is done 50% of an activity period trainning and 50% of the same activity for\n",
    "        #  training. Improve this\n",
    "\n",
    "        if in_accelerometer_position is None:\n",
    "            in_accelerometer_position = ['x_chest', 'x_ankle', 'x_right_arm']\n",
    "\n",
    "        train_test_keys = [f\"x_{key}_train\" for key in in_accelerometer_position] + \\\n",
    "                          [f\"x_{key}_test\" for key in in_accelerometer_position]\n",
    "        final_data_set = {key: [] for key in ['y_label', 'y_onehot', 'user', *train_test_keys]}\n",
    "\n",
    "        for key in in_accelerometer_position:\n",
    "            for data in in_data[f\"x_{key}\"]:\n",
    "                train_data, test_data = self._split_train_test(data, in_points_in_each_set)\n",
    "                final_data_set[f\"x_{key}_train\"].append(train_data)\n",
    "                final_data_set[f\"x_{key}_test\"].append(test_data)\n",
    "\n",
    "        # Label are the same training and test set\n",
    "        final_data_set['y_label'] = in_data['y_label']\n",
    "        final_data_set['y_onehot'] = in_data['y_onehot']\n",
    "        final_data_set['user'] = in_data['user']\n",
    "        final_data_set['y'] = final_data_set['y_onehot']\n",
    "\n",
    "        # And you have a list of keys that you want to keep:\n",
    "        base_keys = ['y', 'y_label', 'y_onehot', 'user']\n",
    "        keys_to_keep = base_keys + [f\"x_{key}_train\" for key in in_accelerometer_position]\n",
    "\n",
    "        # You can create a new dictionary with only these keys as follows:\n",
    "        data_set_train = {k: final_data_set[k] for k in keys_to_keep if k in final_data_set}\n",
    "\n",
    "        keys_to_keep = base_keys + [f\"x_{key}_test\" for key in in_accelerometer_position]\n",
    "\n",
    "        # You can create a new dictionary with only these keys as follows:\n",
    "        data_set_test = {k: final_data_set[k] for k in keys_to_keep if k in final_data_set}\n",
    "\n",
    "        return data_set_train, data_set_test\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analysis = MHealtAnalysis()\n",
    "    analysis.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
