{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from beartype.typing import List, Dict\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "import sys\n",
    "current_path = os.getcwd()\n",
    "sys.path.append(os.path.join(current_path, '../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roug_ml.utl.evaluation.eval_utl import calc_loss_acc_val\n",
    "from roug_ml.utl.evaluation.multiclass import compute_multiclass_confusion_matrix\n",
    "from roug_ml.utl.mlflow_utils import get_best_run\n",
    "from roug_ml.utl.paths_utl import create_dir\n",
    "from roug_ml.models.hyperoptimization import generate_param_grid_with_different_size_layers, \\\n",
    "    get_or_create_experiment, get_best_run_from_hyperoptim\n",
    "from roug_ml.utl.processing.filters import LowPassFilter\n",
    "from roug_ml.utl.processing.signal_processing import SignalProcessor\n",
    "from roug_ml.utl.transforms.features_management import FeatureFlattener\n",
    "from roug_ml.utl.set_seed import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "from roug_ml.models.hyperoptimization import parallele_hyper_optim\n",
    "from roug_ml.utl.parameter_utils import restructure_dict\n",
    "\n",
    "from roug_ml.utl.etl.data_extraction import extract_activity_data_from_users\n",
    "from roug_ml.models.pipelines.pipelines import NNTorch\n",
    "\n",
    "from roug_ml.utl.processing.dataset_processing import covert_3a_from_pandas_to_dict\n",
    "from roug_ml.configs.my_paths import M_HEALTH_PATH\n",
    "from roug_ml.configs.data_labels import M_HEALTH_ACTIVITIES_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = {\n",
    "    \"categorical_crossentropy\": \"categorical_crossentropy\"\n",
    "}\n",
    "\n",
    "\n",
    "def filter_data_dict(data_dict: dict, subjects: list) -> dict:\n",
    "    \"\"\"\n",
    "    Filter the data_dict dictionary based on a specific set of subjects.\n",
    "    :param data_dict: The input dictionary.\n",
    "    :param subjects: The list of subjects to keep.\n",
    "    returns: The filtered dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dict = {\n",
    "        key: [value[i] for i in range(len(data_dict['user'])) if data_dict['user'][i] in subjects]\n",
    "        for key, value in data_dict.items()}\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "class MHealtAnalysis:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Performs following analysis:\n",
    "            - Extract data\n",
    "            - Make plots for analysis\n",
    "        \"\"\"\n",
    "        self.list_of_positions = ['chest', 'ankle', 'right_arm']\n",
    "        # self.list_of_positions = ['right_arm']\n",
    "        self.number_of_point_per_activity = 511  # TODO: automatic\n",
    "\n",
    "        self.features_to_extract = ['mean', 'std', 'rms', 'max', 'min', 'var']\n",
    "\n",
    "        # Define the number of parallel workers\n",
    "        # Use all available CPU cores except one\n",
    "        self.num_workers = 1  # multiprocessing.cpu_count() - 1\n",
    "        self.framework = 'torch'\n",
    "        # self.framework = 'tf'\n",
    "\n",
    "        # model params:\n",
    "        self.nn_params_keys = [\"activations\", \"in_nn\", \"input_shape\", \"output_shape\"]\n",
    "        self.other_keys = [\"batch_size\", \"cost_function\", \"learning_rate\", \"metrics\", \"n_epochs\",\n",
    "                           \"nn_key\"]\n",
    "\n",
    "        self.results_path = os.path.join(M_HEALTH_PATH, '../models')\n",
    "        create_dir(self.results_path)\n",
    "\n",
    "        mlflow.set_tracking_uri(\"http://localhost:8000\")\n",
    "        self.mlflow_experiment_name = 'run_non_parallel_shu_5'\n",
    "        self.mlflow_experiment_id = get_or_create_experiment(self.mlflow_experiment_name)\n",
    "\n",
    "        self.re_optimize = True\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        dataset = self.read_data(M_HEALTH_PATH)\n",
    "\n",
    "        # Process data to right format\n",
    "        dataset_dict_chest = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                           in_label_col=23,\n",
    "                                                           in_user_col=24,\n",
    "                                                           in_3a_col=[0, 1, 2])\n",
    "\n",
    "        dataset_dict_ankle = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                           in_label_col=23,\n",
    "                                                           in_user_col=24,\n",
    "                                                           in_3a_col=[5, 6, 7])\n",
    "\n",
    "        dataset_dict_right_arm = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                               in_label_col=23,\n",
    "                                                               in_user_col=24,\n",
    "                                                               in_3a_col=[14, 15, 16])\n",
    "\n",
    "        dataset_dict = {\n",
    "            'y_num': dataset_dict_chest['y_num'],\n",
    "            'y_onehot': dataset_dict_chest['y_onehot'],\n",
    "            'x_chest': dataset_dict_chest['x'],\n",
    "            'x_ankle': dataset_dict_ankle['x'],\n",
    "            'x_right_arm': dataset_dict_right_arm['x'],\n",
    "            'User': dataset_dict_chest['User']\n",
    "        }\n",
    "\n",
    "        # Create dataset\n",
    "        final_data_set = self.extract_data_during_activity(\n",
    "            in_num_of_users=10,\n",
    "            in_dataset_dict=dataset_dict,\n",
    "            in_accelerometer_position=self.list_of_positions)\n",
    "\n",
    "        data_set_train, data_set_test = self.split_data_for_train_test_portion_of_pa(\n",
    "            in_data=final_data_set, in_points_in_each_set=self.number_of_point_per_activity,\n",
    "            in_accelerometer_position=self.list_of_positions)\n",
    "\n",
    "        # Split by patients: in the original dataset we use\n",
    "        subjects_to_keep = ['subject' + str(i) for i in range(7)]\n",
    "        data_set_train = filter_data_dict(data_set_train, subjects_to_keep)\n",
    "\n",
    "        subjects_to_keep = ['subject' + str(i + 7) for i in range(3)]\n",
    "        data_set_test = filter_data_dict(data_set_test, subjects_to_keep)\n",
    "\n",
    "        key_position = 'x_' + 'right_arm'\n",
    "        # key_position = 'x_' + 'chest'\n",
    "        # key_position = 'x_' + 'ankle'\n",
    "\n",
    "        x_train, y_train_oh = self.create_x_y(data_set_train, in_key=key_position + '_train',\n",
    "                                              in_features_to_extract=self.features_to_extract,\n",
    "                                              sample_freq=50,\n",
    "                                              window_size=1,\n",
    "                                              cutoff_frequency=5\n",
    "                                              )\n",
    "\n",
    "        x_val, y_val_oh = self.create_x_y(data_set_test, in_key=key_position + '_test',\n",
    "                                          in_features_to_extract=self.features_to_extract,\n",
    "                                          sample_freq=50,\n",
    "                                          window_size=1,\n",
    "                                          cutoff_frequency=5)\n",
    "\n",
    "        # Reshape the input data to match the expected shape of the model\n",
    "        x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "        x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "        y = y_train_oh\n",
    "        y_val = y_val_oh\n",
    "\n",
    "        if self.re_optimize:\n",
    "            # Optimize and get best run\n",
    "            params = self.generate_params(x_train=x_train)\n",
    "            results = self.hyperoptimize(x_train, y, x_val, y_val, params)\n",
    "            best_params, best_val_accuracy, best_run_id = get_best_run_from_hyperoptim(results)\n",
    "        else:\n",
    "            # Get the best run from mlopt\n",
    "            mlflow.set_tracking_uri(\"http://localhost:8000\")\n",
    "            best_run_id, best_params = get_best_run(self.mlflow_experiment_name, \"val_accuracy\")\n",
    "\n",
    "            best_params = restructure_dict(best_params, self.nn_params_keys, self.other_keys,\n",
    "                                           in_from_mlflow=True)\n",
    "\n",
    "        # Load the best model\n",
    "        best_model = mlflow.pytorch.load_model(\"runs:/{}/models\".format(best_run_id))\n",
    "\n",
    "        pipeline_torch = Pipeline(steps=[('NN', NNTorch(**best_params))])\n",
    "        pipeline_torch.named_steps['NN'].nn_model = best_model\n",
    "        #\n",
    "        predictions = pipeline_torch.predict(x_val)\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        y_val_binary = np.argmax(y_val, axis=1)\n",
    "        val_acc = calc_loss_acc_val(predictions, y_val_binary)\n",
    "        print(val_acc)\n",
    "        compute_multiclass_confusion_matrix(targets=y_val_binary,\n",
    "                                            outputs=predictions,\n",
    "                                            class_labels=M_HEALTH_ACTIVITIES_LABELS\n",
    "                                            )\n",
    "\n",
    "    def generate_params(self, x_train):\n",
    "        \"\"\"\n",
    "        This function generates the list of parameters for the hyperparameter optimization.\n",
    "\n",
    "        :return: list_params: list of dictionaries, where each dictionary contains a unique\n",
    "         combination of hyperparameters\n",
    "        \"\"\"\n",
    "        # Define hyperparameters\n",
    "        nn_key = ['CNN']\n",
    "        input_shape = [np.asarray(x_train).shape[1]]\n",
    "        output_shape = [13]\n",
    "        batch_size = [  # 32, 64,\n",
    "            128]\n",
    "        cost_function = [loss_functions['categorical_crossentropy']]\n",
    "        learning_rate = [  # 0.01,\n",
    "            0.001]\n",
    "        n_epochs = [  # 300, 150, 140, 130, 120,\n",
    "            100]\n",
    "        metrics = ['accuracy']\n",
    "        layer_sizes = [  # [200, 300], [200, 300, 100], [200, 300, 400, 100], [50, 300, 100, 50],\n",
    "            # [100, 200, 200, 100], [50, 100, 100, 100, 50],\n",
    "            [100, 200, 100, 200, 100]\n",
    "        ]\n",
    "        activations = [\n",
    "            # ['relu', 'relu'], ['relu', 'tanh'], ['relu', 'relu', 'relu'],\n",
    "            # ['tanh', 'tanh', 'tanh', 'tanh'], ['relu', 'relu', 'relu', 'relu'],\n",
    "            # ['relu', 'relu', 'relu', 'relu', 'tanh'],\n",
    "            ['relu', 'relu', 'relu', 'relu', 'relu']\n",
    "        ]\n",
    "        cnn_filters = [1, 3, 5, 10]\n",
    "\n",
    "        list_params = generate_param_grid_with_different_size_layers(nn_key, input_shape,\n",
    "                                                                     output_shape, batch_size,\n",
    "                                                                     cost_function, learning_rate,\n",
    "                                                                     n_epochs, metrics,\n",
    "                                                                     layer_sizes, activations,\n",
    "                                                                     cnn_filters)\n",
    "\n",
    "        list_params = [\n",
    "            restructure_dict(params, self.nn_params_keys, self.other_keys, in_from_mlflow=False) for\n",
    "            params in list_params]\n",
    "\n",
    "        return list_params\n",
    "\n",
    "    def hyperoptimize(self, x_train, y, x_val, y_val, list_params):\n",
    "        \"\"\"\n",
    "        This function performs hyperparameter optimization using parallel computing.\n",
    "\n",
    "        :param x_train: array-like, shape (n_samples, n_features), input training data\n",
    "        :param y: array-like, shape (n_samples, ), target training values\n",
    "        :param x_val: array-like, shape (n_samples, n_features), input validation data\n",
    "        :param y_val: array-like, shape (n_samples, ), target validation values\n",
    "        :param list_params: list of dictionaries, where each dictionary contains a unique\n",
    "        combination of hyperparameters\n",
    "\n",
    "        :return: results: list of tuples, each containing the parameters, validation accuracy, and\n",
    "         run ID for one run of the model\n",
    "        \"\"\"\n",
    "        # Perform hyperparameter optimization\n",
    "        results = parallele_hyper_optim(in_num_workers=self.num_workers,\n",
    "                                        x_train=x_train,\n",
    "                                        y=y,\n",
    "                                        x_val=x_val,\n",
    "                                        y_val=y_val,\n",
    "                                        param_grid_outer=list_params,\n",
    "                                        in_framework=self.framework,\n",
    "                                        model_save_path=self.results_path,\n",
    "                                        in_mlflow_experiment_name=self.mlflow_experiment_id\n",
    "                                        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def create_x_y(final_data_set: dict,\n",
    "                   in_key: str,\n",
    "                   in_features_to_extract: list,\n",
    "                   sample_freq: int,\n",
    "                   window_size: float,\n",
    "                   cutoff_frequency: int):\n",
    "        \"\"\"\n",
    "        Prepare the data by generating the x_data and y_hot_data arrays.\n",
    "        :param final_data_set: The final data set containing the input data.\n",
    "        :param in_key: The key to access the input data in the final_data_set.\n",
    "        :param in_features_to_extract: A list of features to extract from the data. These features\n",
    "                are extracted from final_data_set[in_key] and used as input (x_train).\n",
    "        :param sample_freq: The sample frequency in Hz.\n",
    "        :param window_size: The size of the window in seconds.\n",
    "        :param cutoff_frequency: The cutoff frequency for low-pass filtering.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the x_train and y_train_oh arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        sampling_rate = 1 / sample_freq\n",
    "        points_for_mean = int(window_size / sampling_rate)\n",
    "\n",
    "        dataset_features = []\n",
    "        for x in final_data_set[in_key]:\n",
    "            sp = SignalProcessor(input_signal=x,\n",
    "                                 in_filter=(LowPassFilter, {'cutoff_frequency': cutoff_frequency}),\n",
    "                                 in_window_size=points_for_mean,\n",
    "                                 in_stride=points_for_mean\n",
    "                                 )\n",
    "            sp.apply_filter(in_signal=sp.input_signal)\n",
    "            # sp.calibrate_data(in_signal=sp.output_signal)\n",
    "            features = sp.extract_windowed_features(in_signal=sp.output_signal)\n",
    "            dataset_features.append(features)\n",
    "\n",
    "        flattener = FeatureFlattener(final_data_set, in_features_to_extract)\n",
    "        new_feat = flattener.run(dataset_features)\n",
    "\n",
    "        final_data_set['new_feat'] = np.asarray(new_feat)\n",
    "\n",
    "        x_data = final_data_set['new_feat']\n",
    "        y_hot_data = np.asarray(final_data_set['y'])\n",
    "\n",
    "        return x_data, y_hot_data\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(in_path: str):\n",
    "        \"\"\"\n",
    "        read data\n",
    "        :param in_path: the path\n",
    "        \"\"\"\n",
    "        _, _, filenames = next(os.walk(in_path))\n",
    "        total_df = []\n",
    "        for file_x in filenames:\n",
    "            if file_x.endswith(\".log\"):\n",
    "                print(file_x[8:-4])\n",
    "                # Jutar preprocessed_path \\ file_x\n",
    "                path_to_read = os.path.join(in_path, file_x)\n",
    "                #  leer archivo path_to_read\n",
    "                input_df = pd.read_csv(path_to_read, delimiter=\"\\t\", header=None)\n",
    "                input_df[24] = file_x[8:-4]\n",
    "                total_df.append(input_df[[0, 1, 2, 5, 6, 7, 14, 15, 16, 23, 24]])\n",
    "        return pd.concat(total_df, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_train_test(data: np.array, in_points_in_each_set: int):\n",
    "        \"\"\"\n",
    "        Splits the data into a training set and a test set.\n",
    "\n",
    "        Parameters:\n",
    "        data (np.array): The data to be split.\n",
    "        in_points_in_each_set (int): Number of points in each dataset.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Contains arrays for training data and test data.\n",
    "        \"\"\"\n",
    "        # Convert 3a to image for each position\n",
    "        train_data = data[0:in_points_in_each_set]\n",
    "        test_data = data[in_points_in_each_set + 1: 2 * in_points_in_each_set + 1]\n",
    "        return train_data, test_data\n",
    "\n",
    "    def extract_data_during_activity(self,\n",
    "                                     in_num_of_users: int = 10,\n",
    "                                     in_dataset_dict: Dict = None,\n",
    "                                     in_accelerometer_position: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract data from physical activities periods (Label > 0) from dataset.\n",
    "        :param in_num_of_users: number of patients\n",
    "        :param in_dataset_dict: dictionary with data\n",
    "        :param in_accelerometer_position: position of accelerometer in the body\n",
    "        :return: dictionary final_data_set with train test data.\n",
    "        \"\"\"\n",
    "\n",
    "        data_keys = [f\"x_{key}\" for key in in_accelerometer_position]\n",
    "        final_data_set = {key: [] for key in ['y_label', 'y_onehot', 'user', *data_keys]}\n",
    "\n",
    "        list_of_classes = in_dataset_dict['y_num']\n",
    "        for label in np.unique(list_of_classes):\n",
    "            # Number of users\n",
    "            list_user = ['subject' + str(i + 1) for i in range(in_num_of_users)]\n",
    "            for user_i in list_user:\n",
    "                user_data_dict = extract_activity_data_from_users(\n",
    "                    user_i, label, in_dataset_dict, self.list_of_positions)\n",
    "\n",
    "                for key in in_accelerometer_position:\n",
    "                    final_data_set[f\"x_{key}\"].append(user_data_dict[f\"{key}_label\"])\n",
    "\n",
    "                final_data_set['y_label'].append(label)\n",
    "                final_data_set['y_onehot'].append(user_data_dict['y_onehot'][0])\n",
    "                final_data_set['user'].append(user_i)\n",
    "        return final_data_set\n",
    "\n",
    "    def split_data_for_train_test_portion_of_pa(self,\n",
    "                                                in_data: Dict,\n",
    "                                                in_points_in_each_set: int = int(1022 / 2),\n",
    "                                                in_accelerometer_position=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Split data for training and test sets. in_points_in_each_set for training and\n",
    "        in_points_in_each_set for testing. Data is already a dict of lists where x[0] is a set of 3a\n",
    "        date that correspond to label y_label[0] and its one_hot_version (y_onehot[0]). Therefore,\n",
    "        the only variable that is processed (split) here is x\n",
    "        :param in_data: dictionary with data\n",
    "        :param in_points_in_each_set: int, number of points in the dataset\n",
    "        :param in_accelerometer_position: position of accelerometer in the body\n",
    "        :return: dictionary with train test data.\n",
    "        \"\"\"\n",
    "        # TODO: split is done 50% of an activity period trainning and 50% of the same activity for\n",
    "        #  training. Improve this\n",
    "\n",
    "        if in_accelerometer_position is None:\n",
    "            in_accelerometer_position = ['x_chest', 'x_ankle', 'x_right_arm']\n",
    "\n",
    "        train_test_keys = [f\"x_{key}_train\" for key in in_accelerometer_position] + \\\n",
    "                          [f\"x_{key}_test\" for key in in_accelerometer_position]\n",
    "        final_data_set = {key: [] for key in ['y_label', 'y_onehot', 'user', *train_test_keys]}\n",
    "\n",
    "        for key in in_accelerometer_position:\n",
    "            for data in in_data[f\"x_{key}\"]:\n",
    "                train_data, test_data = self._split_train_test(data, in_points_in_each_set)\n",
    "                final_data_set[f\"x_{key}_train\"].append(train_data)\n",
    "                final_data_set[f\"x_{key}_test\"].append(test_data)\n",
    "\n",
    "        # Label are the same training and test set\n",
    "        final_data_set['y_label'] = in_data['y_label']\n",
    "        final_data_set['y_onehot'] = in_data['y_onehot']\n",
    "        final_data_set['user'] = in_data['user']\n",
    "        final_data_set['y'] = final_data_set['y_onehot']\n",
    "\n",
    "        # And you have a list of keys that you want to keep:\n",
    "        base_keys = ['y', 'y_label', 'y_onehot', 'user']\n",
    "        keys_to_keep = base_keys + [f\"x_{key}_train\" for key in in_accelerometer_position]\n",
    "\n",
    "        # You can create a new dictionary with only these keys as follows:\n",
    "        data_set_train = {k: final_data_set[k] for k in keys_to_keep if k in final_data_set}\n",
    "\n",
    "        keys_to_keep = base_keys + [f\"x_{key}_test\" for key in in_accelerometer_position]\n",
    "\n",
    "        # You can create a new dictionary with only these keys as follows:\n",
    "        data_set_test = {k: final_data_set[k] for k in keys_to_keep if k in final_data_set}\n",
    "\n",
    "        return data_set_train, data_set_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject7\n",
      "subject10\n",
      "subject6\n",
      "subject4\n",
      "subject5\n",
      "subject1\n",
      "subject2\n",
      "subject3\n",
      "subject8\n",
      "subject9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'validation_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m analysis \u001b[38;5;241m=\u001b[39m MHealtAnalysis()\n\u001b[0;32m----> 2\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 129\u001b[0m, in \u001b[0;36mMHealtAnalysis.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mre_optimize:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Optimize and get best run\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_params(x_train\u001b[38;5;241m=\u001b[39mx_train)\n\u001b[0;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperoptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     best_params, best_val_accuracy, best_run_id \u001b[38;5;241m=\u001b[39m get_best_run_from_hyperoptim(results)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Get the best run from mlopt\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 214\u001b[0m, in \u001b[0;36mMHealtAnalysis.hyperoptimize\u001b[0;34m(self, x_train, y, x_val, y_val, list_params)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mThis function performs hyperparameter optimization using parallel computing.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m run ID for one run of the model\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter optimization\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallele_hyper_optim\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_num_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                                \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mx_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                                \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mparam_grid_outer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m                                \u001b[49m\u001b[43min_framework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                \u001b[49m\u001b[43min_mlflow_experiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlflow_experiment_id\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/DiaHecDev/pa_recognition/roug_ml/scripts/../../roug_ml/models/hyperoptimization.py:60\u001b[0m, in \u001b[0;36mparallele_hyper_optim\u001b[0;34m(in_num_workers, x_train, y, x_val, y_val, param_grid_outer, in_framework, model_save_path, in_mlflow_experiment_name)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39min_num_workers)(\n\u001b[1;32m     55\u001b[0m         delayed(process_params)(params_outer, x_train, y, x_val, y_val)\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m params_outer \u001b[38;5;129;01min\u001b[39;00m param_grid_outer\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Perform parallel processing for Torch framework\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_num_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_params_torch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43min_mlflow_experiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams_outer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam_grid_outer\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/DiaHecDev/pa_recognition/roug_ml/scripts/../../roug_ml/models/hyperoptimization.py:164\u001b[0m, in \u001b[0;36mprocess_params_torch\u001b[0;34m(params, x_train, y, x_val, y_val, model_save_path, in_mlflow_experiment_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mProcess parameters and compute validation accuracy for Torch pipeline.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m:return: A tuple of the processed parameters, validation accuracy, and run ID.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 164\u001b[0m pipeline_torch, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_fit_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    167\u001b[0m pipeline_torch\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msave(model_save_path, model_name)\n",
      "File \u001b[0;32m~/DiaHecDev/pa_recognition/roug_ml/scripts/../../roug_ml/models/hyperoptimization.py:132\u001b[0m, in \u001b[0;36mcreate_and_fit_pipeline\u001b[0;34m(params, x_train, y, x_val, y_val)\u001b[0m\n\u001b[1;32m    127\u001b[0m pipeline_torch \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m, NNTorch2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams))])\n\u001b[1;32m    128\u001b[0m params_pipeline \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    129\u001b[0m     pipeline_torch\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__validation_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: (x_val\u001b[38;5;241m.\u001b[39mreshape(x_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    130\u001b[0m                                                                       x_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    131\u001b[0m                                                         y_val)}\n\u001b[0;32m--> 132\u001b[0m \u001b[43mpipeline_torch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams_pipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline_torch, pipeline_torch\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mval_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/tensorflow-metal/lib/python3.8/site-packages/sklearn/pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    381\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 382\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'validation_data'"
     ]
    }
   ],
   "source": [
    "analysis = MHealtAnalysis()\n",
    "analysis.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
